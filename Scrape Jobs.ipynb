{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scrape careers page for top Startups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import unicode_literals\n",
    "from bs4 import BeautifulSoup\n",
    "from contextlib import closing\n",
    "from requests import get\n",
    "from requests.exceptions import RequestException\n",
    "import matplotlib.pyplot as plt\n",
    "import csv\n",
    "import html5lib\n",
    "import logging\n",
    "import os\n",
    "import pandas as pd\n",
    "import re\n",
    "import sys\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.basicConfig(filename = '/home/hardik/H-1B-Data-Analysis/scraping_logs.log', filemode='a', level=logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger = logging.getLogger('test')\n",
    "logger.setLevel(logging.INFO)\n",
    "logger.addHandler(logging.StreamHandler(sys.stdout))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_html(url):\n",
    "    \"\"\"This function makes a GET Request to the url specified.\n",
    "    This gets the raw HTML content if there is a good response.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        resp = get(url, stream=True, timeout=30)\n",
    "        if resp.status_code == 200:\n",
    "            return resp.content\n",
    "        else:\n",
    "            return None\n",
    "    except Exception as e:\n",
    "        logging.exception(str(e))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(parsed_data_html):\n",
    "    \n",
    "    # Convert html data into beautifulSoup format using html parser\n",
    "    parsed_data_bs = BeautifulSoup(parsed_data_html, \"html.parser\")\n",
    "\n",
    "    # Search for table with id = 'myTable' and extract each row of data\n",
    "    parsed_data_class = parsed_data_bs.find('div', {'class': 'article-content'})\n",
    "    parsed_data_list = parsed_data_class.find('ol')\n",
    "    list_items = parsed_data_list.findAll('li')\n",
    "    extracted_items = []\n",
    "    for item in list_items:\n",
    "        extracted_items.append(item.text)\n",
    "    return extracted_items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://techcrunch.com/2019/10/02/these-are-the-top-y-combinator-companies-of-all-time-based-on-valuation/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "parsed_data_html = get_html(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_startups = get_data(parsed_data_html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "for index in range(len(top_startups)):\n",
    "    if '\\xa0 ' in top_startups[index]:\n",
    "        top_startups[index] = top_startups[index].replace('\\xa0 ', '').lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_startups = [company.lower() for company in top_startups]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.info('-'.join(top_startups))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get careers page for each company if it exists\n",
    "##### Create a list of valid results and invalid results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_results(companies):\n",
    "    logger = logging.getLogger('results')\n",
    "    logger.setLevel(logging.INFO)\n",
    "    logger.addHandler(logging.StreamHandler(sys.stdout))\n",
    "    valid_company, invalid_company, responses = [], [], [] \n",
    "    for company in companies:\n",
    "        logger.info(company)\n",
    "        url = 'https://www.' + company.replace(' ','') + '.com/careers'\n",
    "        response = get_html(url)\n",
    "        if response:\n",
    "            valid_company.append(company)\n",
    "            responses.append(response)\n",
    "        else:\n",
    "            responses.append('')\n",
    "            invalid_company.append(company)\n",
    "    return valid_company, invalid_company, responses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "stripe\n",
      "airbnb\n",
      "cruise\n",
      "doordash\n",
      "coinbase\n",
      "instacart\n",
      "dropbox\n",
      "ginkgo bioworks\n",
      "gusto\n",
      "flexport\n",
      "rappi\n",
      "brex\n",
      "reddit\n",
      "gitlab\n",
      "pagerduty\n",
      "checkr\n",
      "segment\n",
      "docker\n",
      "scale\n",
      "faire\n",
      "twitch\n",
      "plangrid\n",
      "mixpanel\n",
      "amplitude\n",
      "optimizely\n",
      "boom supersonic\n",
      "grin\n",
      "meesho\n",
      "algolia\n",
      "goat\n",
      "zapier\n",
      "messagebird\n",
      "standard cognition\n",
      "memebox\n",
      "embark\n",
      "helion energy\n",
      "equipmentshare\n",
      "sendbird\n",
      "rescale\n",
      "gocardless\n",
      "rigetti computing\n",
      "razorpay\n",
      "north\n",
      "relativity space\n",
      "podium\n",
      "benchling\n",
      "ironclad\n",
      "newfront\n",
      "influxdata\n",
      "webflow\n",
      "people.ai\n",
      "weebly\n",
      "xendit\n",
      "matterport\n",
      "easypost\n",
      "sift\n",
      "the athletic\n",
      "mattermost\n",
      "wepay\n",
      "vidyard\n",
      "weave\n",
      "nurx\n",
      "proxy\n",
      "heap\n",
      "payfazz\n",
      "memsql\n",
      "fivetran\n",
      "rippling\n",
      "clever\n",
      "heroku\n",
      "fivestars\n",
      "coreos\n",
      "cleartax\n",
      "quero education\n",
      "ridecell\n",
      "hellosign\n",
      "grubmarket\n",
      "lattice\n",
      "unbabel\n",
      "athelas inc.\n",
      "oh my green\n",
      "lever\n",
      "atrium\n",
      "zeus\n",
      "front\n",
      "le tote\n",
      "shipbob\n",
      "snapdocs\n",
      "gitprime\n",
      "scribd\n",
      "guesty\n",
      "axoni\n",
      "lob\n",
      "notable\n",
      "atomwise\n",
      "flutterwave\n",
      "panorama education\n",
      "futureadvisor\n",
      "sfox\n",
      "lambda school\n",
      "zerodown\n"
     ]
    }
   ],
   "source": [
    "valid_company_list, invalid_company_list, responses = get_results(top_startups)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60, 41)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(valid_company_list), len(invalid_company_list)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
